<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>如何让大语言模型调用外部工具？MCP技术解析与批判 | lijianfei.com</title>
<link rel="shortcut icon" href="https://lijianfei.com/favicon.ico?v=1761120781074">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://lijianfei.com/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="如何让大语言模型调用外部工具？MCP技术解析与批判 | lijianfei.com - Atom Feed" href="https://lijianfei.com/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">
<script id="chatway" async="true" src="https://cdn.chatway.app/widget.js?id=laNLrbQJYsup"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5152652314157130"
     crossorigin="anonymous">
</script>

<script type="text/javascript">
    (function(c,l,a,r,i,t,y){
        c[a]=c[a]||function(){(c[a].q=c[a].q||[]).push(arguments)};
        t=l.createElement(r);t.async=1;t.src="https://www.clarity.ms/tag/"+i;
        y=l.getElementsByTagName(r)[0];y.parentNode.insertBefore(t,y);
    })(window, document, "clarity", "script", "m0j4678d23");
</script>
     


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-74398720-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-74398720-1');
</script>


    <meta property="og:title" content="如何让大语言模型调用外部工具？MCP技术解析与批判">
    <meta property="og:description" content="想象一下，你问一个所谓“智能”的大语言模型（LLM）：“截止到今天，周杰伦有多少粉丝？”它会一脸茫然地盯着你，然后甩出一堆过时数据——毕竟，它不过是被一堆陈旧文本喂出来的“复读机”，连今天的日期都搞不清，更别提实时粉丝数了。可笑吗？有点。可...">
    <meta property="og:type" content="MCP,AI agent,AI">
    <meta property="og:url" content="https://lijianfei.com/post/ru-he-rang-da-yu-yan-mo-xing-diao-yong-wai-bu-gong-ju-mcp-ji-zhu-jie-xi-yu-pi-pan/">
    <meta property="og:image" content="https://lijianfei.com/post-images/ru-he-rang-da-yu-yan-mo-xing-diao-yong-wai-bu-gong-ju-mcp-ji-zhu-jie-xi-yu-pi-pan.png"/>
    <meta property="og:image:width" content="800">
    <meta property="og:image:height" content="500">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@lijianfei_com">
    <meta name="twitter:title" content="如何让大语言模型调用外部工具？MCP技术解析与批判">
    <meta name="twitter:image" content="https://lijianfei.com/post-images/ru-he-rang-da-yu-yan-mo-xing-diao-yong-wai-bu-gong-ju-mcp-ji-zhu-jie-xi-yu-pi-pan.png">
    <meta name="twitter:image:width" content="800">

    <meta name="description" content="想象一下，你问一个所谓“智能”的大语言模型（LLM）：“截止到今天，周杰伦有多少粉丝？”它会一脸茫然地盯着你，然后甩出一堆过时数据——毕竟，它不过是被一堆陈旧文本喂出来的“复读机”，连今天的日期都搞不清，更别提实时粉丝数了。可笑吗？有点。可..." />
    <meta name="keywords" content="MCP,AI agent,AI" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="https://cdn.bootcss.com/highlight.js/9.12.0/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://lijianfei.com">
  <img class="avatar" src="https://lijianfei.com/images/avatar.png?v=1761120781074" alt="">
  </a>
  <h1 class="site-title">
    lijianfei.com
  </h1>
  <p class="site-description">
    思危，思退，思变。<br>
追求进步，不求完美。<br>
做好小事，熬过难事，静成大事。<br>
语以泄败，事以密成，沉得住气，静得下心。
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu">
          关于我
        </a>
      
    
      
        <a href="https://github.com/lijianfeigeek" class="menu" target="_blank">
          Github
        </a>
      
    
  </div>
  <div class="social-container">
    
      
    
      
    
      
    
      
    
      
    
    <img src="https://ghchart.rshah.org/F15406/lijianfeigeek" style="max-width: 100%;height: auto;"/>
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              如何让大语言模型调用外部工具？MCP技术解析与批判
            </h2>
            <div class="post-info">
              <span>
                2025-04-10
              </span>
              <span>
                6 min read
              </span>
              
                <a href="https://lijianfei.com/tag/mcp/" class="post-tag">
                  # MCP
                </a>
              
                <a href="https://lijianfei.com/tag/ai-agent/" class="post-tag">
                  # AI agent
                </a>
              
                <a href="https://lijianfei.com/tag/ai/" class="post-tag">
                  # AI
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://lijianfei.com/post-images/ru-he-rang-da-yu-yan-mo-xing-diao-yong-wai-bu-gong-ju-mcp-ji-zhu-jie-xi-yu-pi-pan.png" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content">
                <p>想象一下，你问一个所谓“智能”的大语言模型（LLM）：“截止到今天，周杰伦有多少粉丝？”它会一脸茫然地盯着你，然后甩出一堆过时数据——毕竟，它不过是被一堆陈旧文本喂出来的“复读机”，连今天的日期都搞不清，更别提实时粉丝数了。可笑吗？有点。可悲吗？更有点。</p>
<!-- more -->
<p>但如果你读完这篇文章，至少能让这个“复读机”稍微聪明一点，甚至还能为你加个粉丝——前提是你愿意动手试试。</p>
<p>本文将拆解如何让LLM调用外部工具，解决它与现实世界脱节的尴尬。让我们从技术的角度出发，顺便带点批判的眼光，看看这背后的逻辑与陷阱。</p>
<h2 id="1-问题本质llm的先天缺陷">1. 问题本质：LLM的先天缺陷</h2>
<h3 id="11-静态的智商">1.1 静态的“智商”</h3>
<p>LLM的核心问题在于，它的“知识”是被冻结在训练数据里的。比如，你想知道某个实时数据，它只能给你抛出一个“对不起，我的知识截止到2023年”之类的废话。为什么？因为它本质上是个封闭系统，与外部世界没有实时连接。这就像一个自以为是的学者，书读得不少，却从不出门看看世界变了没有。</p>
<h3 id="12-外部工具救命稻草">1.2 外部工具：救命稻草？</h3>
<p>要让LLM回答实时问题，比如“轩辕有多少粉丝”，最直接的办法是给它接上外部工具——比如一个爬虫程序，实时抓取数据。但问题来了：LLM运行在云端，你的爬虫在本地，它俩隔着互联网的“长城”，怎么沟通？直接调用？做梦吧。</p>
<h2 id="2-技术解法一function-calling">2. 技术解法一：Function Calling</h2>
<h3 id="21-中介程序的翻译角色">2.1 中介程序的“翻译”角色</h3>
<p>既然LLM没法直接访问本地工具，那就找个“中介”帮忙。这个中介程序通过API与LLM对话，充当桥梁。流程大致如下：</p>
<ol>
<li><strong>中介上线</strong>：告诉LLM，“我这儿有个爬虫工具，能干啥，怎么用，需要的话吱一声。”</li>
<li><strong>LLM分析</strong>：收到问题后，判断是否需要工具，生成调用指令。</li>
<li><strong>中介执行</strong>：按指令调用爬虫，拿到结果，再扔回给LLM。</li>
<li><strong>LLM包装</strong>：把结果加工成自然语言，丢给你。</li>
</ol>
<p>听起来不错吧？但这套流程的核心在于，如何让LLM和中介“对上话”。</p>
<h3 id="22-openai的标准化说明书">2.2 OpenAI的标准化“说明书”</h3>
<p>早期，大家直接把工具说明写在提示词里，简单粗暴。可惜，LLM的“智商”有限，你白纸黑字写清楚了，它还是可能满嘴跑火车，返回一堆乱码。于是，OpenAI推出了Function Calling技术，用JSON格式标准化工具描述，单独与用户输入分开。返回时，LLM用特定字段明确指出要调用啥工具、带啥参数。</p>
<p>优点显而易见：清晰、规范，LLM不容易“犯浑”。缺点呢？每个工具都要手写JSON“说明书”，费时费力。更别提，像Defi这样的平台，虽然能把HTTP接口转成Function Calling格式，但配置起来依旧繁琐——几十上百个工具，你愿意一个个敲吗？</p>
<h3 id="23-安全隐患">2.3 安全隐患</h3>
<p>还有个问题：工具直接暴露HTTP接口，等于把后门敞开给黑客。效率提高了，安全却打了折扣。这让我想起一句老话：“技术是把双刃剑，用得好是生产力，用不好是自杀器。”</p>
<h2 id="3-技术解法二mcp模型上下文协议">3. 技术解法二：MCP（模型上下文协议）</h2>
<h3 id="31-封装一切的壳">3.1 封装一切的“壳”</h3>
<p>Function Calling的麻烦和安全问题催生了MCP。它的思路很简单：既然每个工具都得费劲描述，那就封装一层“壳”，统一管理。外部工具不再直接暴露，而是被包装成MCP Server；中介程序里的MCP Client负责和Server通信。这套架构的核心组件是：</p>
<ul>
<li><strong>MCP Server</strong>：工具的“外壳”，提供标准接口。</li>
<li><strong>MCP Client</strong>：中介的“触手”，与Server对接。</li>
<li><strong>MCP Host</strong>：中介的“大本营”，比如VS Code插件。</li>
</ul>
<h3 id="32-通信方式">3.2 通信方式</h3>
<p>MCP支持两种通信模式：</p>
<ol>
<li><strong>标准输入输出流</strong>：Client和Server在同一台机器上时，用进程间通信，效率高。</li>
<li><strong>HTTP SSE</strong>：跨机器时，用服务器推送事件（SSE），你看AI一个字一个字蹦出来那种效果，就是SSE的功劳。</li>
</ol>
<p>协议基于JSON RPC 2.0，格式统一，扩展性强。</p>
<h3 id="33-优势与代价">3.3 优势与代价</h3>
<p>MCP的杀手锏是简单：用SDK加个装饰器，几行代码就搞定工具开发，不用写Web Server，也不用手敲API Schema。安全上，统一管理也减少了直接暴露的风险。但代价呢？我做了个实验，发现MCP Client与LLM通信时，直接把6万多字符的工具说明塞进提示词，烧Token烧得心疼。这效率，简直像用跑车拉煤。</p>
<h2 id="4-实验揭秘mcp如何运转">4. 实验揭秘：MCP如何运转</h2>
<h3 id="41-实验设置">4.1 实验设置</h3>
<p>我写了个简单的MCP Server，内置两个工具：获取文章列表和视频列表。Client跑在VS Code插件里，连接本地8000端口，用抓包工具（Easy Teach Shark）记录通信。</p>
<h3 id="42-通信过程">4.2 通信过程</h3>
<ol>
<li><strong>Client发起</strong>：发JSON RPC请求，指定工具和参数。</li>
<li><strong>Server响应</strong>：通过SSE推送结果，干净利落。</li>
</ol>
<p>但与LLM的通信让我傻眼：Client没用Function Calling，而是把工具说明一股脑塞进提示词，AI再根据约定格式返回调用指令。6万字符的提示词，堪称“Token杀手”。</p>
<h2 id="5-对比与反思">5. 对比与反思</h2>
<h3 id="51-function-calling-vs-mcp">5.1 Function Calling vs. MCP</h3>
<ul>
<li><strong>Function Calling</strong>：标准化但繁琐，安全堪忧。</li>
<li><strong>MCP</strong>：开发简单、安全性高，但与LLM通信原始，效率低下。</li>
</ul>
<h3 id="52-技术与权力">5.2 技术与权力</h3>
<p>从Function Calling到MCP，技术在封装和抽象中进步。但这背后还有个隐秘问题：谁控制这些工具？OpenAI的标准化像极了中心化的“技术霸权”，而MCP的灵活性则更偏向去中心化。可惜，当前实现还不够优雅，未来能否真正打破垄断，值得观察。</p>
<h2 id="6-结语">6. 结语</h2>
<p>让LLM调用外部工具，看似技术问题，实则关乎信息自由。Function Calling和MCP各有千秋，但都不完美。技术的发展从来不是直线前进，而是充满妥协与试错。正如“编程随想”所言：“技术无罪，但用它的人有心。”在追逐效率时，别忘了问问自己：这工具，到底服务于谁？</p>
<p>读到这里，你是选择盲从厂商的方案，还是动手试试自己的爬虫？独立思考，从来比盲目的“三连”更有价值。</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#1-%E9%97%AE%E9%A2%98%E6%9C%AC%E8%B4%A8llm%E7%9A%84%E5%85%88%E5%A4%A9%E7%BC%BA%E9%99%B7">1. 问题本质：LLM的先天缺陷</a>
<ul>
<li><a href="#11-%E9%9D%99%E6%80%81%E7%9A%84%E6%99%BA%E5%95%86">1.1 静态的“智商”</a></li>
<li><a href="#12-%E5%A4%96%E9%83%A8%E5%B7%A5%E5%85%B7%E6%95%91%E5%91%BD%E7%A8%BB%E8%8D%89">1.2 外部工具：救命稻草？</a></li>
</ul>
</li>
<li><a href="#2-%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%B3%95%E4%B8%80function-calling">2. 技术解法一：Function Calling</a>
<ul>
<li><a href="#21-%E4%B8%AD%E4%BB%8B%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%BF%BB%E8%AF%91%E8%A7%92%E8%89%B2">2.1 中介程序的“翻译”角色</a></li>
<li><a href="#22-openai%E7%9A%84%E6%A0%87%E5%87%86%E5%8C%96%E8%AF%B4%E6%98%8E%E4%B9%A6">2.2 OpenAI的标准化“说明书”</a></li>
<li><a href="#23-%E5%AE%89%E5%85%A8%E9%9A%90%E6%82%A3">2.3 安全隐患</a></li>
</ul>
</li>
<li><a href="#3-%E6%8A%80%E6%9C%AF%E8%A7%A3%E6%B3%95%E4%BA%8Cmcp%E6%A8%A1%E5%9E%8B%E4%B8%8A%E4%B8%8B%E6%96%87%E5%8D%8F%E8%AE%AE">3. 技术解法二：MCP（模型上下文协议）</a>
<ul>
<li><a href="#31-%E5%B0%81%E8%A3%85%E4%B8%80%E5%88%87%E7%9A%84%E5%A3%B3">3.1 封装一切的“壳”</a></li>
<li><a href="#32-%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F">3.2 通信方式</a></li>
<li><a href="#33-%E4%BC%98%E5%8A%BF%E4%B8%8E%E4%BB%A3%E4%BB%B7">3.3 优势与代价</a></li>
</ul>
</li>
<li><a href="#4-%E5%AE%9E%E9%AA%8C%E6%8F%AD%E7%A7%98mcp%E5%A6%82%E4%BD%95%E8%BF%90%E8%BD%AC">4. 实验揭秘：MCP如何运转</a>
<ul>
<li><a href="#41-%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE">4.1 实验设置</a></li>
<li><a href="#42-%E9%80%9A%E4%BF%A1%E8%BF%87%E7%A8%8B">4.2 通信过程</a></li>
</ul>
</li>
<li><a href="#5-%E5%AF%B9%E6%AF%94%E4%B8%8E%E5%8F%8D%E6%80%9D">5. 对比与反思</a>
<ul>
<li><a href="#51-function-calling-vs-mcp">5.1 Function Calling vs. MCP</a></li>
<li><a href="#52-%E6%8A%80%E6%9C%AF%E4%B8%8E%E6%9D%83%E5%8A%9B">5.2 技术与权力</a></li>
</ul>
</li>
<li><a href="#6-%E7%BB%93%E8%AF%AD">6. 结语</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">下一篇</div>
            <a href="https://lijianfei.com/post/guan-tian-feng-yi-ge-hei-ke-de-jue-qi-yu-qi-shi/">
              <h3 class="post-title">
                关天峰：黑客界“天高我为峰”
              </h3>
            </a>
          </div>
        

        

        <div class="site-footer">
  Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
  <a class="rss" href="https://lijianfei.com/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
